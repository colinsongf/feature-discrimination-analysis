{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats, optimize\n",
    "from pandas import DataFrame, Series\n",
    "import seaborn as sns\n",
    "import random as rd\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import scipy.stats\n",
    "import patsy\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import basinhopping\n",
    "from sklearn import linear_model\n",
    "import multiprocessing\n",
    "##Code for analysis of fMRI experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read in data\n",
    "data_dir = os.path.abspath('../')\n",
    "all_rts = pd.read_csv(data_dir + '/all_rts.csv', index_col =0)\n",
    "exp_order = pd.read_csv(data_dir + '/exp_order.csv', index_col =0)\n",
    "\n",
    "subjects = list(np.loadtxt(data_dir+ '/subjects.txt',str))\n",
    "\n",
    "#some experiment variables\n",
    "order_dict_ser = {'c_plus':6, 'b_plus':2, 'b_minus':3, 'c_minus':5} #coding for trial order vector\n",
    "order_dict_sim = {'c_plus':4, 'b_plus':1, 'b_minus':2, 'c_minus':3}\n",
    "# exp_conditions = ['c_plus','b_plus','b_minus','c_minus'] #coding for trial order vector\n",
    "order_dict_exp = {'ser': order_dict_ser, 'sim':order_dict_sim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#process the RTs a bit (remove trial types 1, 4, and 10 (see below) and mean center)\n",
    "ser_rts = all_rts[all_rts['exp']=='ser']\n",
    "ser_rts = ser_rts[ser_rts['order'] != 10] #ITI\n",
    "ser_rts = ser_rts[ser_rts['order'] != 1] #A\n",
    "ser_rts = ser_rts[ser_rts['order'] != 4] #A\n",
    "\n",
    "sim_rts = all_rts[all_rts['exp']=='sim']\n",
    "sim_rts = sim_rts[sim_rts['order'] != 10] #ITI\n",
    "\n",
    "rt_dict_exp = {'ser': ser_rts, 'sim':sim_rts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def perform_RL(trial_order,rew_trial,alpha,exp):\n",
    "    V = {'b_plus':[0], 'b_minus' : [0], 'c_plus' : [0],'c_minus':[0]}\n",
    "    delta = {'b_plus':[], 'b_minus' : [], 'c_plus' : [],'c_minus':[]}\n",
    "    index = {'b_plus':[], 'b_minus' : [], 'c_plus' : [],'c_minus':[]}\n",
    "    order_dict = order_dict_exp[exp]\n",
    "    count = 0\n",
    "    for n,cond in enumerate(trial_order):\n",
    "        trial_type = None\n",
    "        for key in order_dict:\n",
    "            if order_dict[key] == cond:\n",
    "                trial_type = key\n",
    "                \n",
    "        if trial_type is not None:\n",
    "            rew = rew_trial[n]\n",
    "            delta[trial_type].append(rew - V[trial_type][-1]) #compute PE\n",
    "            new_V = V[trial_type][-1] + alpha * delta[trial_type][-1] #calculate V for next trial\n",
    "            V[trial_type].append(new_V) \n",
    "            index[trial_type].append(count)\n",
    "            count += 1 \n",
    "\n",
    "    return V, delta, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##function for building dataframe of relevant data for each subject\n",
    "def build_df(sub,num_runs,exp):\n",
    "    \n",
    "    alpha = .1 #just some arbitrary alpha to get things going\n",
    "    order_dict = order_dict_exp[exp]\n",
    "    predictors = {'V':[],'PE':[],'run':[], 'cond':[], 'trial_index':[],'rt':[]}\n",
    "    \n",
    "    for i in range(1,num_runs+1):\n",
    "        ##perform RL for the experimental condition\n",
    "        event_order = exp_order[exp_order['exp']==exp]\n",
    "        rew = np.array(event_order[event_order['run']==i]['rew'])\n",
    "        tt = np.array(event_order[event_order['run']==i]['trial_order'])\n",
    "        V, delta, index = perform_RL(tt,rew,alpha,exp)\n",
    "\n",
    "        #get rt data for this subject and run\n",
    "        rts = rt_dict_exp[exp]\n",
    "        rt_data = rts[(rts['sub']==sub) & (rts['run']==i) ]\n",
    "        ##update predictors dict\n",
    "        for key in V.keys():\n",
    "            V[key] = V[key][:-1] #last entry is for subsequent trial that doesnt exist\n",
    "            predictors['V'].extend(V[key])\n",
    "            predictors['PE'].extend(delta[key])\n",
    "            predictors['cond'].extend([key]*len(V[key]))\n",
    "            predictors['run'].extend(['run' + str(i)]*len(V[key]))\n",
    "            predictors['trial_index'].extend(index[key])\n",
    "            predictors['rt'].extend(rt_data[rt_data['order']==key]['rt'].values)\n",
    "\n",
    "    predictors = pd.DataFrame(predictors)\n",
    "    predictors = predictors.sort(['run','trial_index']) #get predictors in proper order\n",
    "\n",
    "    #Z score RT\n",
    "    predictors['rt'] = predictors['rt'] - predictors['rt'].mean()\n",
    "    predictors['rt'] = predictors['rt'] / predictors['rt'].std()\n",
    "        \n",
    "    #Z-score trial index for each row\n",
    "    for i in range(1,num_runs+1):\n",
    "        run = 'run' + str(i)\n",
    "        predictors.loc[predictors['run']==run,'trial_index'] =  predictors.loc[predictors['run']==run,'trial_index'] - \\\n",
    "        predictors.loc[predictors['run']==run,'trial_index'].mean()\n",
    "        predictors.loc[predictors['run']==run,'trial_index'] =  predictors.loc[predictors['run']==run,'trial_index'] / \\\n",
    "        predictors.loc[predictors['run']==run,'trial_index'].std()\n",
    "    \n",
    "    return predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##returns loss for linear regression\n",
    "def regress(params,predictors,num_runs,exp):\n",
    "    \n",
    "    alpha = scipy.stats.logistic.cdf(params[0]) #learning rate\n",
    "    beta = params[1:]\n",
    "    \n",
    "    ##perform RL for the experimental condition\n",
    "    for i in range(1,num_runs+1):\n",
    "        event_order = exp_order[exp_order['exp']==exp]\n",
    "        rew = np.array(event_order[event_order['run']==i]['rew'])\n",
    "        tt = np.array(event_order[event_order['run']==i]['trial_order'])\n",
    "        V, delta, index = perform_RL(tt,rew,alpha,exp)\n",
    "        \n",
    "        for key in V.keys():\n",
    "            V[key] = V[key][:-1] #last entry is for subsequent trial that doesnt exist\n",
    "            predictors.loc[(predictors['run'] == 'run' + str(i)) & (predictors['cond'] == key),'V'] = V[key]\n",
    "            predictors.loc[(predictors['run'] == 'run' + str(i)) & (predictors['cond'] == key),'PE'] = delta[key]\n",
    "\n",
    "            \n",
    "    #build RL matrixes\n",
    "    yd,Xd = patsy.dmatrices(\"rt ~ 1+trial_index+V+run\",predictors,NA_action='drop')\n",
    "    print Xd.design_info\n",
    "    X = np.asarray(Xd)\n",
    "    y=np.array(map(float,np.asarray(yd)))\n",
    "\n",
    "    #compute prediction and loss\n",
    "    y_hat = np.dot(X,beta)\n",
    "    loss = np.linalg.norm(y - y_hat)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = (0,0,0,0,0,0) #initialize parameters to 0\n",
    "#res = minimize(regress,params,method='BFGS', options={'disp': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##loop through all subjects\n",
    "def run_subjects(sub_id):\n",
    "    exp = 'ser'\n",
    "    #get predictors dataframe\n",
    "    num_runs = len(np.unique(ser_rts[ser_rts['sub']==sub_id]['run']))\n",
    "    predictors = build_df(sub_id,num_runs,exp)\n",
    "\n",
    "    #initialize\n",
    "    params = (0,0,0,0,0,0) #initialize parameters to 0\n",
    "\n",
    "    #minimize\n",
    "    res = minimize(regress,params,args=(predictors,num_runs,exp),method='BFGS', options={'disp': True})\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 6.509029\n",
      "         Current function value: 5.700052\n",
      "         Current function value: 6.443897\n",
      "         Current function value: 7.067638\n",
      "         Current function value: 7.013071\n",
      "         Current function value: 6.535289\n",
      "         Current function value: 6.284968\n",
      "         Current function value: 5.781786\n",
      "         Current function value: 6.673548\n",
      "         Current function value: 7.024720\n",
      "         Current function value: 6.417378\n",
      "         Current function value: 7.036001\n",
      "         Current function value: 6.585944\n",
      "         Current function value: 6.184457\n",
      "         Current function value: 4.013122\n",
      "         Current function value: 7.440861\n",
      "         Current function value: 6.306080\n",
      "         Current function value: 7.046454\n",
      "         Current function value: 7.107934\n",
      "         Current function value: 5.768380\n",
      "         Current function value: 7.329464\n",
      "         Current function value: 7.444888\n",
      "         Current function value: 6.971967\n",
      "         Current function value: 6.421189\n",
      "         Current function value: 7.110278\n",
      "         Current function value: 6.537499\n",
      "         Current function value: 5.157899\n",
      "         Current function value: 6.210744\n",
      "         Iterations: 12\n",
      "         Iterations: 13\n",
      "         Iterations: 13\n",
      "         Iterations: 15\n",
      "         Iterations: 14\n",
      "         Iterations: 14\n",
      "         Iterations: 14\n",
      "         Iterations: 15\n",
      "         Iterations: 14\n",
      "         Iterations: 16\n",
      "         Iterations: 14\n",
      "         Iterations: 15\n",
      "         Iterations: 17\n",
      "         Iterations: 15\n",
      "         Iterations: 19\n",
      "         Iterations: 21\n",
      "         Iterations: 22\n",
      "         Iterations: 23\n",
      "         Iterations: 26\n",
      "         Iterations: 31\n",
      "         Iterations: 35\n",
      "         Iterations: 44\n",
      "         Iterations: 161\n",
      "         Iterations: 171\n",
      "         Iterations: 189\n",
      "         Iterations: 199\n",
      "         Iterations: 270\n",
      "         Iterations: 270\n",
      "         Function evaluations: 120\n",
      "         Function evaluations: 128\n",
      "         Function evaluations: 128\n",
      "         Function evaluations: 136\n",
      "         Function evaluations: 136\n",
      "         Function evaluations: 136\n",
      "         Function evaluations: 136\n",
      "         Function evaluations: 136\n",
      "         Function evaluations: 144\n",
      "         Function evaluations: 144\n",
      "         Function evaluations: 144\n",
      "         Function evaluations: 144\n",
      "         Function evaluations: 152\n",
      "         Function evaluations: 152\n",
      "         Function evaluations: 184\n",
      "         Function evaluations: 200\n",
      "         Function evaluations: 200\n",
      "         Function evaluations: 216\n",
      "         Function evaluations: 248\n",
      "         Function evaluations: 272\n",
      "         Function evaluations: 336\n",
      "         Function evaluations: 440\n",
      "         Function evaluations: 1708\n",
      "         Function evaluations: 1852\n",
      "         Function evaluations: 2148\n",
      "         Function evaluations: 2148\n",
      "         Function evaluations: 2892\n",
      "         Function evaluations: 3016\n",
      "         Gradient evaluations: 15\n",
      "         Gradient evaluations: 16\n",
      "         Gradient evaluations: 16\n",
      "         Gradient evaluations: 17\n",
      "         Gradient evaluations: 17\n",
      "         Gradient evaluations: 17\n",
      "         Gradient evaluations: 17\n",
      "         Gradient evaluations: 17\n",
      "         Gradient evaluations: 18\n",
      "         Gradient evaluations: 18\n",
      "         Gradient evaluations: 18\n",
      "         Gradient evaluations: 18\n",
      "         Gradient evaluations: 19\n",
      "         Gradient evaluations: 19\n",
      "         Gradient evaluations: 23\n",
      "         Gradient evaluations: 25\n",
      "         Gradient evaluations: 25\n",
      "         Gradient evaluations: 27\n",
      "         Gradient evaluations: 31\n",
      "         Gradient evaluations: 34\n",
      "         Gradient evaluations: 42\n",
      "         Gradient evaluations: 55\n",
      "         Gradient evaluations: 212\n",
      "         Gradient evaluations: 230\n",
      "         Gradient evaluations: 267\n",
      "         Gradient evaluations: 267\n",
      "         Gradient evaluations: 360\n",
      "         Gradient evaluations: 377\n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool(processes=30)\n",
    "output = pool.map(run_subjects,subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.02201493e+00   4.20986086e-01   7.09594824e-02   1.23652362e-01\n",
      "   2.92311840e-02  -5.83978764e+01]\n",
      "[  5.80542378e+00   4.90343762e-01   3.70467114e-01   4.96102811e-01\n",
      "   1.76900832e-01   2.67976949e+02]\n",
      "-1.26530123314\n",
      "0.891315890696\n",
      "(-6.5053689026617691, 1.9113774817830713e-06)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAECCAYAAAAxVlaQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116121f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = []\n",
    "for n,o in enumerate(output):\n",
    "    if o.success:\n",
    "        params.append(o.x)    \n",
    "params = np.array(params)\n",
    "\n",
    "print np.mean(params, axis=0)\n",
    "print np.std(params,axis = 0)\n",
    "\n",
    "V_beta = [b for b in params[:,5] if b>-200]\n",
    "print np.mean(V_beta)\n",
    "print np.std(V_beta)\n",
    "plt.hist(V_beta)\n",
    "print scipy.stats.ttest_1samp(V_beta,0)\n",
    "# plt.hist(map(lambda x: scipy.stats.logistic.cdf(x),params[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -3.52411454, -13.38725855,  11.71133923,  -9.9083914 ,  24.31352455])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#double check that linear regression code works (given a fixed alpha)\n",
    "clf = linear_model.LinearRegression(fit_intercept=False)\n",
    "clf.fit(X, y)\n",
    "clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now try to fit all subjects' data simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##function for building dataframe of relevant data for each subject\n",
    "def build_df_all(num_runs, exp):\n",
    "    order_dict = order_dict_exp[exp]\n",
    "    alpha = .1\n",
    "    predictors = {'subject':[],'V':[],'PE':[],'run':[], 'cond':[], 'trial_index':[],'rt':[]}\n",
    "\n",
    "    for i in range(1,num_runs+1):\n",
    "\n",
    "        ##perform RL for the experimental condition\n",
    "        event_order = exp_order[exp_order['exp']==exp]\n",
    "        rew = np.array(event_order[event_order['run']==i]['rew'])\n",
    "        tt = np.array(event_order[event_order['run']==i]['trial_order'])\n",
    "        V, delta, index = perform_RL(tt,rew,alpha,exp)\n",
    "        for key in V.keys():\n",
    "            V[key] = V[key][:-1] #last entry is for subsequent trial that doesnt exist\n",
    "\n",
    "        for sub in subjects:\n",
    "            sub_id = get_sub_id(sub)\n",
    "    \n",
    "            #get rt data for this subject and run\n",
    "            rts = rt_dict_exp[exp]\n",
    "            rt_data = rts[(rts['sub']==sub_id) & (rts['run']==i) ]\n",
    "\n",
    "            ##update predictors dict\n",
    "            for key in V.keys():\n",
    "                predictors['V'].extend(V[key])\n",
    "                predictors['PE'].extend(delta[key])\n",
    "                predictors['cond'].extend([key]*len(V[key]))\n",
    "                predictors['subject'].extend([sub]*len(V[key]))\n",
    "                predictors['run'].extend(['run' + str(i)]*len(V[key]))\n",
    "                predictors['trial_index'].extend(index[key])\n",
    "                predictors['rt'].extend(rt_data[rt_data['order']==key]['rt'].values)\n",
    "\n",
    "    predictors = pd.DataFrame(predictors)\n",
    "    predictors = predictors.sort(['subject','run','trial_index']) #get predictors in proper order\n",
    "\n",
    "    #mean center RT\n",
    "#     predictors['rt'] = predictors['rt'] - predictors['rt'].mean()\n",
    "    \n",
    "    #Z score RT for each subject\n",
    "    for sub in subjects:\n",
    "        predictors.loc[predictors['subject']==sub,'rt'] =  predictors.loc[predictors['subject']==sub,'rt'] - \\\n",
    "        predictors.loc[predictors['subject']==sub,'rt'].mean()  \n",
    "        predictors.loc[predictors['subject']==sub,'rt'] =  predictors.loc[predictors['subject']==sub,'rt'] / \\\n",
    "        predictors.loc[predictors['subject']==sub,'rt'].std() \n",
    "        \n",
    "    #Z-score trial index for each row\n",
    "    for i in range(1,num_runs+1):\n",
    "        run = 'run' + str(i)\n",
    "        predictors.loc[predictors['run']==run,'trial_index'] =  predictors.loc[predictors['run']==run,'trial_index'] - \\\n",
    "        predictors.loc[predictors['run']==run,'trial_index'].mean()\n",
    "        predictors.loc[predictors['run']==run,'trial_index'] =  predictors.loc[predictors['run']==run,'trial_index'] / \\\n",
    "        predictors.loc[predictors['run']==run,'trial_index'].std()\n",
    "    \n",
    "    return predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##returns loss for linear regression\n",
    "def regress_all(params,predictors,num_runs,exp):\n",
    "    \n",
    "    alpha = scipy.stats.logistic.cdf(params[0]) #learning rate\n",
    "    beta = params[1:]\n",
    "\n",
    "    ##perform RL for the experimental condition\n",
    "    for i in range(1,num_runs+1):\n",
    "        event_order = exp_order[exp_order['exp']==exp]\n",
    "        rew = np.array(event_order[event_order['run']==i]['rew'])\n",
    "        tt = np.array(event_order[event_order['run']==i]['trial_order'])\n",
    "        V, delta, index = perform_RL(tt,rew,alpha,exp)\n",
    "        for key in V.keys():\n",
    "            V[key] = V[key][:-1] #last entry is for subsequent trial that doesnt exist \n",
    "            predictors.ix[(predictors['run'] == 'run' + str(i)) & (predictors['cond'] == key), 'V'] = V[key]*len(subjects)\n",
    "            predictors.ix[(predictors['run'] == 'run' + str(i)) & (predictors['cond'] == key), 'PE'] = delta[key]*len(subjects)\n",
    "    \n",
    "    #build RL matrixes\n",
    "    yd,Xd = patsy.dmatrices(\"rt ~ 1+trial_index+V+run\",predictors,NA_action='drop')\n",
    "    X = np.asarray(Xd)\n",
    "    y=np.array(map(float,np.asarray(yd)))\n",
    "\n",
    "    #compute prediction and loss\n",
    "    y_hat = np.dot(X,beta)\n",
    "    loss = np.linalg.norm(y - y_hat)\n",
    "    return loss\n",
    "# regress_all(params,predictors,num_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 37.446722\n",
      "         Iterations: 15\n",
      "         Function evaluations: 152\n",
      "         Gradient evaluations: 19\n"
     ]
    }
   ],
   "source": [
    "exp = 'ser'\n",
    "params = [0]\n",
    "params.extend(np.zeros(5))\n",
    "num_runs  = 3\n",
    "predictors = build_df_all(num_runs,exp)\n",
    "loss = regress_all(params,predictors,num_runs,exp)\n",
    "# bounds = [(0,1)]\n",
    "# for i in range(0,5):\n",
    "#     bounds.append((None,None))\n",
    "\n",
    "#minimize\n",
    "res = minimize(regress_all,params,args=(predictors,num_runs,exp),method='BFGS', options={'disp': True})\n",
    "# minimizer_kwargs = {\"method\": \"L-BFGS-B\",'bounds':bounds,'args':(predictors,num_runs,exp)}\n",
    "# ret = basinhopping(regress_all, params, minimizer_kwargs=minimizer_kwargs,niter=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   status: 0\n",
      "  success: True\n",
      "     njev: 19\n",
      "     nfev: 152\n",
      " hess_inv: array([[  5.50651905e+00,  -3.07922673e-01,   2.33267479e-02,\n",
      "         -5.94927013e-02,  -4.75468318e-01,   2.08926805e+00],\n",
      "       [ -3.07922673e-01,   1.42649614e-01,  -7.40082290e-02,\n",
      "         -6.84640981e-02,   3.89242964e-02,  -2.47989508e-01],\n",
      "       [  2.33267479e-02,  -7.40082290e-02,   1.40476483e-01,\n",
      "          7.18888574e-02,  -6.67026668e-03,   1.40855404e-02],\n",
      "       [ -5.94927013e-02,  -6.84640981e-02,   7.18888574e-02,\n",
      "          1.43918324e-01,   2.52228032e-03,  -1.81637030e-02],\n",
      "       [ -4.75468318e-01,   3.89242964e-02,  -6.67026668e-03,\n",
      "          2.52228032e-03,   6.87146314e-02,  -2.08121203e-01],\n",
      "       [  2.08926805e+00,  -2.47989508e-01,   1.40855404e-02,\n",
      "         -1.81637030e-02,  -2.08121203e-01,   1.10509382e+00]])\n",
      "      fun: 37.44672203113975\n",
      "        x: array([-0.47898567,  0.3620377 ,  0.12028519,  0.15692983,  0.00447625,\n",
      "       -1.11605283])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "      jac: array([  0.00000000e+00,   1.90734863e-06,   4.76837158e-07,\n",
      "         1.43051147e-06,  -1.90734863e-06,   4.76837158e-07])\n",
      "0.382491672398\n"
     ]
    }
   ],
   "source": [
    "print res\n",
    "print scipy.stats.logistic.cdf(res.x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  nfev: 7196\n",
       " minimization_failures: 0\n",
       "                   fun: 37.446722031145107\n",
       "                     x: array([ 0.38249334,  0.36203726,  0.12028474,  0.15693011,  0.00447598,\n",
       "       -1.11605044])\n",
       "               message: ['requested number of basinhopping iterations completed successfully']\n",
       "                   nit: 50"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  status: 0\n",
       " success: True\n",
       "    nfev: 182\n",
       "     fun: 37.710933119262073\n",
       "       x: array([ 0.25209737,  0.16060626,  0.25796919,  0.38188396,  0.08537042,\n",
       "       -1.05208584])\n",
       " message: 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
       "     jac: array([  3.55271368e-06,   7.10542736e-07,  -2.84217094e-06,\n",
       "        -2.13162821e-06,   5.68434189e-06,   0.00000000e+00])\n",
       "     nit: 20"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  nfev: 8344\n",
       " minimization_failures: 0\n",
       "                   fun: 37.710933119262073\n",
       "                     x: array([ 0.25209737,  0.16060626,  0.25796919,  0.38188396,  0.08537042,\n",
       "       -1.05208584])\n",
       "               message: ['requested number of basinhopping iterations completed successfully']\n",
       "                   nit: 50"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#update experiment file with PE regressor made from the best fitting alpha\n",
    "best_alphas = {'ser':.38, 'sim':.25}\n",
    "timing_info = pd.read_csv(data_dir + '/timing_info.csv', index_col =0)\n",
    "\n",
    "for exp in best_alphas.keys():\n",
    "    \n",
    "    alpha = best_alphas[exp]\n",
    "    \n",
    "    ##perform RL for the experimental condition\n",
    "    for i in range(1,num_runs+1):\n",
    "        event_order = exp_order[exp_order['exp']==exp]\n",
    "        rew = np.array(event_order[event_order['run']==i]['rew'])\n",
    "        tt = np.array(event_order[event_order['run']==i]['trial_order'])\n",
    "        V, delta, index = perform_RL(tt,rew,alpha,exp)\n",
    "        \n",
    "        for key in V.keys():\n",
    "            deltas = delta[key]\n",
    "            deltas = list(deltas - np.mean(deltas)) #mean center\n",
    "            timing_info.ix[(timing_info['run'] == i) & (timing_info['condition'] == key) & \\\n",
    "                                 (timing_info['exp']==exp),'PE'] = deltas*len(subjects)\n",
    "timing_info.to_csv(data_dir + '/timing_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-292-b63b0f7e3297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtiming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'timing' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
